The History of Physical User Interfaces
User interfaces have evolved from abstract text screens to rich, tangible experiences that feel alive. Over
decades, designers have increasingly drawn inspiration from the physical world – using familiar metaphors,
realistic motion, and even simulated physics – to make digital interactions more intuitive and delightful.
Below is a chronological journey through major UI milestones, from the earliest command lines to today’s
“liquid” and material designs that blur the line between digital and physical.
1960s–1970s: Command Lines – Abstract Beginnings
In the early days, using a computer meant typing commands on a black-and-white terminal. These
command-line interfaces (CLI) were powerful but completely abstract – just text with no visual metaphors.
Users had to memorize exact commands, and there was no concept of a “physical” object on screen. The
interaction was strictly cognitive, not visual or tactile. This era set the stage by highlighting what was
missing: interfaces that the average person could grasp more naturally.
1980s: The GUI Revolution – Desktops and Direct Manipulation
The late 1970s and 1980s introduced the graphical user interface (GUI), which fundamentally changed
how we interact with computers. Research at Xerox PARC led to the first GUI machine, the Xerox Alto
(1973), which pioneered the desktop metaphor 1
. For the first time, digital information was represented
as icons on a virtual “desktop”, analogous to documents on a physical desk. Files looked like paper sheets,
folders like manila file folders, and there was even a trash can icon for deleting – users could drag a file to
the trash, just like throwing away a paper 1
. This made computing far more concrete: people could
interact via direct manipulation, clicking and dragging on-screen objects as if they were real.
An Apple Lisa 1 (1983) showcasing its GUI. Early GUIs introduced a “desktop” with icons (documents, folders, a
trash bin, etc.) and windows, making software feel analogous to a physical workspace 1
. Users could directly
manipulate these elements with a mouse – a radical shift from typed commands.
Building on PARC’s ideas, Apple’s Lisa (1983) and Macintosh (1984) brought GUIs to a wider audience. They
refined the desktop metaphor and popularized WIMP interfaces (windows, icons, menus, pointer). The
Mac’s smiling computer and trash can icon instantly communicated their purpose, leveraging skeuomorphic
design – using familiar real-world imagery to make new technology friendly. This era established that visual
metaphors and pointer-based interaction could make computing intuitive. As Apple’s GUI showed, you didn’t
need to learn arcane commands; you could point, click, and drag objects on screen with immediate, visible
results. This directness made the interface feel more physical even though it was on a screen.
1990s: Refinement – Skeuomorphism and Familiar Visuals
By the 1990s, GUIs were mainstream on desktop computers (e.g. Windows 3.1, Windows 95, Mac System 7).
Designers continuously refined UIs to be more visually rich and familiar. A key approach was
1
2
skeuomorphism – making UI elements look like real objects to leverage users’ existing knowledge .
Software icons and controls often had 3D shading, textures, and details imitating physical materials. For
example, Microsoft’s GUI introduced beveled buttons that looked “raised” or “pressed,” and Apple’s apps
featured illustrations like notepads with torn paper edges or address books that looked like leather-bound
books. These ornamental cues weren’t functionally necessary, but they made digital tools feel tangible and
friendly, helping users recognize what things were for (Don Norman noted that skeuomorphic cues can
strongly suggest an object’s affordances – how you can interact with it).
Skeuomorphic design reached an apex in the late ’90s and early 2000s, when interfaces became quite
ornamental. Apple’s QuickTime player resembled a physical stereo, complete with knobs and faux metallic
sheen. Real-world metaphors were everywhere: a notepad app looked like yellow legal paper, and the
recycle bin icon in Windows actually “filled up” with crumpled paper as you deleted files. These touches gave
UIs a comforting, familiar feel. Studies later found that such realistic icons can make navigation faster, since
they’re easier to recognize than abstract graphics. In short, the 1990s showed that visual familiarity and a
hint of the physical world could lower the learning curve of software.
At the same time, fundamental interaction models were solidified. Drag-and-drop became common (e.g.
dragging files to folders or trash), reinforcing the illusion of manipulating physical objects. This directness
was highly intuitive – as UI guru Ben Shneiderman described, the best interfaces allow users to act on
objects directly rather than through complex commands. The GUI had turned computing into an interactive
visual environment, and each evolution added a bit more of the real world into the digital experience.
Early 2000s: Polishing the Experience – Animation and “Real”
Feedback
As computing power grew, UIs gained smooth animations and visual effects that made interactions feel
more continuous and physical. Rather than instant, jarring changes, interfaces began to animate
transitions in a way that mimics real-world motion (providing continuity for the user’s mental model). For
example, windows didn’t just pop open or closed – they could minimize with a genie effect (as in Mac OS X),
visually shrinking and “whooshing” into the Dock. This gave a visual cue of where the window went, almost
like seeing an object slide into a drawer.
Around 2001, Apple’s Mac OS X Aqua interface introduced a glossy, “liquid” look – with gel-like buttons,
reflective surfaces, and translucent elements that resembled glass. Scrollbars and icons appeared to have
a water-like sheen. These aesthetics, while playful, also served to make the interface feel touchable and
modern. Aqua’s translucency (e.g. see-through title bars) layered content in a subtle 3D stack, hinting at
depth. Microsoft followed a few years later: Windows Vista (2006) debuted the Aero design, which gave
every window a translucent glass frame with blurred backgrounds, and added fluid animations for
minimizing, closing, and switching windows. The entire UI felt like it was made of glass panels floating in
space – a clear attempt to add depth and physicality to the otherwise flat desktop. (Observers at the time
noted that Aero’s frosted-glass look was a major stylistic shift.) These effects made the screen less of a static
plane and more of a layered, moving environment.
Another 2000s innovation was incorporating basic physics into desktop UI elements for fun and intuition.
Linux systems with 3D compositing (Compiz, circa 2006) famously allowed “wobbly windows” – when you
dragged a window, it behaved like a jiggling sheet of rubber. While mostly cosmetic, it gave an almost
2
subconscious cue that the window was a pliable object. Researchers even experimented with full 3D
desktop environments: BumpTop (2007), for example, turned your desktop into a pseudo-physics sandbox
where icons were like paper cards on a desk that could be shuffled, stacked, or flicked around with realistic
collisions. BumpTop was a conceptual leap combining a literal physical representation (a messy desk) with
digital benefits like instant sorting. These experiments demonstrated an appetite for more tangible,
playful interactions beyond the rigid 2D grid of icons.
Under the hood, the 2000s saw user interface frameworks embracing GPU acceleration, enabling richer
visuals without slowing down computers 3
. This allowed for those drop shadows, soft edges, and smoothly
animated transitions that we now take for granted. UI animation principles (as later codified by Apple’s and
Google’s guidelines) were increasingly about respecting physics: for instance, an object might ease in/out
with acceleration curves that feel natural to the eye, rather than linear movements. All these touches –
subtle shadows, glossy materials, animated transitions – were bringing software closer to our physical reality,
inch by inch.
Late 2000s: Touchscreens & Mobile – Physics at Your Fingertips
A seismic shift in “physical” UI interaction arrived with smartphones, especially the original iPhone in 2007.
For the first time, users could directly touch and manipulate UI elements with their fingers, not just a mouse
pointer. This multi-touch interface brought a whole new level of natural interaction – you could pinch to
zoom (like grabbing and resizing a photo with two fingers) or swipe to scroll, using the same gestures you
might with real objects. Software responded in kind by adopting real-world physics behaviors. Notably,
the iPhone introduced inertial scrolling and “rubber-band” edges in its UI. If you flicked a list or webpage,
it would scroll smoothly and then decelerate, as if it had mass and momentum. And when you hit the end of
the content, it wouldn’t just stop abruptly – the view would stretch slightly and bounce back like a spring,
providing satisfying feedback that you’d reached the limit. These small physics-based details made the
interface feel alive. Users instantly understood these cues (you can almost feel the elasticity), which tapped
into our innate understanding of how objects behave in the real world.
Touchscreens also popularized gestural interfaces. Swiping, dragging, long-pressing – these mimic
physical manipulations (e.g. swiping is like pushing something aside). The slide-to-unlock mechanism on
early iPhones literally looked like a switch that you slide with your finger, complete with a tactile groove
graphic. Mobile UIs of the late 2000s and early 2010s were rife with skeuomorphic designs to ease the
transition for billions of new users. Apple’s iPhone OS (later iOS) up to version 6 (2007–2012) was filled with
richly textured UIs: the Notes app had a yellow legal pad background, the Calendar looked like a leather-
bound desk calendar, and even the Voice Memos app icon was a picture of a classic microphone. These
visuals were static, but provided instant familiarity (“ah, that’s a notebook”). Apple took skeuomorphism to
an extreme – for example, Game Center had a green felt casino-table motif. The philosophy was to make
the function obvious through mimesis: a bookshelf of ebooks looked like a wooden shelf with book covers
on it, so users felt at home.
Importantly, mobile also brought motion and feedback to a new level. Early iPhones and iPads included
subtle dynamic cues: when you deleted an email, a little paper trash bin lid would flip open to “swallow” it.
Some on-screen controls responded to device tilt – for instance, on iPad, the shine on a metallic slider knob
would shift as you moved the device, simulating real reflective metal. These touches, while decorative,
signaled a trend: using device sensors and animations to add responsive realism to UI. Mobile devices also
3
introduced haptic feedback (vibrations on touch) to simulate tactile responses, further physicalizing the
experience of tapping a flat glass screen.
All told, the late 2000s and early 2010s, led by smartphones, cemented that physics-based motion and
touch-friendly design can hugely enhance usability. An intuitive, natural feel became a hallmark of quality.
As an example, Steve Jobs famously highlighted the iPhone’s fluid scrolling and gestures as something
“magical” – and it was the underlying physics and direct manipulation that made it so. Users now expected
interfaces to respond like real objects: scrolling lists should glide and bounce, buttons should depress, and
apps should open with cinematic flair. The era proved that engaging our sense of touch and motion could
make software more compelling and user-friendly.
2010s: Flat Design and Beyond – From Skeuomorphic to Material
While skeuomorphism had helped onboard new users, by the early 2010s it started to feel overdone – UIs
were sometimes so skeuomorphic they felt cluttered or kitschy. A major design pivot came with flat design.
Companies began stripping away heavy textures and ornamentation in favor of minimalist, flat visuals.
Microsoft was an early leader here: its Metro design language (introduced with Windows Phone 7 in 2010)
deliberately avoided skeuomorphism. Metro used bold typography, simple icons, and “content over
chrome” – i.e. emphasizing content instead of ornamental frames. It relied on clean lines and solid colors,
bringing a modern, print-like clarity. Importantly, Metro also brought in motion as a key element:
transitions and swipes in Windows Phone were smooth and friendly, giving a sense of a living system even
without faux-3D effects.
The flat design trend reached a tipping point in 2013 when Apple released iOS 7 – a complete redesign of
the iPhone’s UI. Skeuomorphic textures were banished in favor of flat icons, thin fonts, and a very minimal,
“digital” aesthetic. The visual change was dramatic (no more leather, wood, or drop-shadows). But
interestingly, Apple replaced those static cues with dynamic, physics-driven ones. iOS 7 embraced the
idea of layers and depth through motion 4
. For example, it introduced translucent frosted-glass panels
(for Control Center, Notification Center, etc.) – these blurred the content beneath, creating a sense of
a sheet of glass overlaying the interface. It also used a parallax effect on the Home Screen: as you tilted
the phone, the app icons and background image shifted subtly, as if the icons were on a plane above the
wallpaper 4
. This made the otherwise flat UI feel multi-layered and spatial. In Apple’s words, iOS 7 was “an
entire aesthetic” but with “distinct visual layers” to establish hierarchy and context 4
. In short, flat design
didn’t mean the UIs became completely 2D and static – instead, designers found new ways (like real-time
blur, lighting, and motion) to convey physicality.
In 2014, Google introduced Material Design, which can be seen as a resolution to the flat vs. skeuomorphic
debate. Material Design embraced flat graphic style but explicitly built in a material metaphor: it treated UI
elements as if they were paper sheets and ink in a physical space. Google’s designers said “the material is
grounded in tactile reality” – essentially imagining the interface as layers of paper stacked with consistent
lighting and shadows. Buttons and cards would have subtle drop shadows to indicate which layer is on top,
and they would “lift” and float* when touched, much like a real piece of paper being picked up. Material
Design also emphasized *meaningful motion: animations were not just for show, but to provide
continuity and feedback as users navigated. For example, tapping a list item could expand it into a full-
screen card, with the motion carefully choreographed so the element appears to grow from exactly where
you tapped – maintaining a sense of tangible continuity. Material Design even set up physics-like rules:
elements shouldn’t teleport or violate the laws of motion; instead, transitions were smooth transformations
4
within a single environment. This was essentially a physics model for UI, using concepts like consistent
easing curves (acceleration/deceleration) to make every movement feel natural. By blending flat visuals
with real-world physics and lighting, Material Design proved that UIs can be both clean and object-like. As
the spec put it, familiar tactile cues help users “quickly understand affordances” (what they can do), while
the flexible digital “material” can do things real paper can’t – but without breaking the rules of physics.
During this period, Microsoft also evolved its design language. By 2017, they introduced the Fluent Design
System, which reintroduced translucency (called Acrylic material) and emphasis on light and depth across
Windows 10. Hovering your mouse might produce a reveal highlight – as if a light is shining through the
interface, illuminating buttons under your cursor. These effects gave UI elements a sense of material
presence (buttons weren’t just flat color; they reacted to focus and movement). Fluent’s principles – light,
depth, motion, material, scale – align closely with the industry trend of making digital interfaces behave
more like physical objects in a real environment.
Meanwhile, designers and developers started exploring new modes of interaction. Voice-controlled
interfaces (Siri, Alexa, etc.) emerged in the 2010s, and while not “physical UI” in a visual sense, they aimed
for naturalness by using conversational speech. In the realm of visuals, early augmented reality (AR) apps
and devices appeared, blending digital info with the physical world. Google Glass (2013) and Microsoft
HoloLens (2016) projected UIs into the user’s environment, requiring a rethink of UI as something that
could float in 3D space. These were harbingers of a coming era where spatial, 3D interfaces would become
more common.
By the late 2010s, flat design was no longer novel – in fact, it risked making everything look too similar and
sterile. Users and designers began to crave more personality and tactile feedback again. Google’s Material
Design 2 updates and Material Theming (around 2018) allowed for more customization – different shapes,
typography, and colors – injecting some warmth and brand personality back into apps. On the Apple side,
iOS gradually brought back subtle depth (for instance, iOS 13’s Cards UI for modals, which appear as
rounded sheets that slide up, casting a shadow). Even in web and mobile design communities, there was
talk of “neumorphism” (circa 2020) – a style that combined flat design with soft, extruded shadows so that
UI elements looked like they were embossed in the interface. While neumorphism was mostly a Dribbble
fad (and criticized for poor accessibility), its popularity signaled a desire to make interfaces feel touchable
and not completely flat.
In summary, the 2010s swung the pendulum from skeuomorphic richness to ultra-flat minimalism, and
then started to settle on a happy medium: clean, scalable design enhanced by motion, depth, and
physics-based cues. Users grew accustomed to interfaces that not only looked simpler, but responded to
them in more human ways – with smooth motions and subtle acknowledgment of their actions. The stage
was set for the next era, combining the best of both worlds.
2020s: The Tangible Interface – Spatial, Alive, and Adaptive
Today, UI design is coming full-circle to embrace physicality in a deeper sense than ever – not through
heavy skeuomorphic textures, but through responsive motion, depth, and even immersion in 3D space.
A number of converging trends define the 2020s:
Spatial Computing and AR/VR: Advances in hardware have brought augmented reality and virtual
reality interfaces to the forefront. AR/VR aim to literally integrate digital UI into our physical environment.
5
For example, in a VR headset or AR glasses, menus and windows can be placed in 3D space around the user,
often with realistic lighting and physics. Apple’s announcement of the Vision Pro in 2023, dubbed a “spatial
computer,” exemplifies this shift. In visionOS (the operating system for Vision Pro), app windows are
rendered as if they’re real objects floating in your room – they cast shadows, they have parallax as you
move, and you can pinch in the air to grab or resize them. This is a direct extension of the UI metaphor
into physical space. Similarly, AR interfaces on smartphones (think Pokémon GO or AR navigation in Google
Maps) overlay digital elements onto the camera view of the real world. These interfaces must obey some
physics (staying anchored to real-world positions, scaling properly with distance, etc.) to be convincing and
usable. The rise of spatial computing has reinforced the idea that 3D, physically aware UI is not just for
niche applications – it’s becoming a mainstream user experience. We now have to consider Z-axis
placement, real-world scale, and how virtual objects interact with the environment (e.g. occlusion and
collision) as part of interface design.
Modern Material Design (Material You & Expressive): Google has continued to evolve Material Design,
pushing it further towards expressiveness and motion. In 2021, Material You (Material Design 3)
introduced dynamic color personalization (making the UI adapt to the user’s wallpaper and preferences),
which made interfaces feel more personal if not physical. Then in 2023–2024, Google unveiled Material 3
Expressive mode – effectively an enhancement that infuses more emotion and physics into Android’s UI.
Material 3 Expressive uses a new motion physics system (spring-based animations) to make interactions
feel “alive, fluid, and natural”. Instead of simple fades or static movements, UI elements in Expressive
mode bounce and react with springy behavior – for example, navigating or dismissing elements triggers
playful yet coherent motions as though the components have a bit of weight and flexibility. Google’s
research found that these physics-inspired animations not only delight users but also improve usability
(users were able to identify interface changes faster with expressive, moving designs). In short, Material 3
Expressive is reviving principles of motion and depth to end the era of overly-flat, “soulless” app designs. It
adds features like background blurs (creating depth layers), more fluid transitions, and even support for
spatial components for AR/VR contexts. This shows Google preparing Android for a future where screens
might be foldable, head-worn, or otherwise more immersive – ensuring the design system can scale into 3D
and respond physically.
Apple’s Liquid Glass Design: One of the most striking recent milestones is Apple’s new “Liquid Glass”
design language announced at WWDC 2025. This is Apple’s biggest interface redesign in a decade, and it
explicitly focuses on making UI elements tangible and dynamic across all Apple platforms. “Liquid Glass” is
described as a “new material” for the interface – a translucent, fluid glass-like layer that all controls and
windows are made of. In practice, this means that buttons, sidebars, and windows on the latest iOS, iPadOS,
macOS, etc., have a glassy appearance that reflects and refracts the environment behind them, and they
morph in response to user interaction. For example, a toolbar made of Liquid Glass might shrink or
expand fluidly as you scroll content, almost like a living object adapting to focus your attention. Colors of
the UI aren’t fixed – they are “informed by surrounding content”, meaning the interface picks up hues from
your wallpaper or app content, as if the glass is truly transparent. When you move a window or invoke a
menu, Liquid Glass elements even show specular highlights and dynamic lighting, as if light is glinting off
a physical piece of glass. All of this contributes to a sense of depth and vitality: the UI feels less like flat
pixels and more like a collection of objects with substance. Apple notes that Liquid Glass was “inspired by the
depth and dimensionality of visionOS” (their AR platform), underscoring how spatial computing influences
conventional UI. The goal is to make interactions “more fun and magical” by giving even simple actions a
bit of spectacle and realism. Importantly, Apple implemented this in a way that spans across devices – from
iPhones to Macs to the Watch – signaling a unified push toward physics-driven, context-aware UI at the
6
core of their design. It’s not just visual either: Apple built Liquid Glass with real-time rendering on Apple’s
GPUs, so the effects are smooth, and they’ve accounted for accessibility (ensuring, for instance, that
transparency doesn’t impede readability and that users can reduce motion if needed). In essence, Liquid
Glass wraps the Apple ecosystem in a fresh coat of digital material that behaves with the elegance of real
glass and fluid.
Xerox Alto (1973) – the first GUI – and Apple’s Liquid Glass (2025). In the Xerox Alto’s era, UI elements were simple
and static. Today’s Liquid Glass UI uses real-time blur, lighting, and fluid animation so interface elements behave
like tangible objects. It’s a culmination of decades of design moving toward physical realism in software.
Expressiveness vs. Minimalism: The latest design trends highlight an industry consensus that adding
physics-based motion and visual depth can make interfaces more engaging without sacrificing usability.
In fact, research indicates these lively, human-centered touches improve both user satisfaction and task
performance 5
. Where the 2010s prized flat minimalism (sometimes to a fault), the mid-2020s are
embracing what Google calls “expressive” design – interfaces that have personality and emotion. This
doesn’t mean a return to skeuomorphic clutter; it means using modern technology (powerful GPUs, high-
refresh displays, sensors, etc.) to create UIs that respond to users like real-world objects and environments
would. A button might feel more substantial through subtle animation and haptic feedback, a list might
bounce to signal you’ve reached the end (instead of a hard stop), and an app might adapt its look and
layout to different contexts seamlessly (like changing theme with your environment or smoothly
reconfiguring in AR).
In designing these new interfaces, there’s also a focus on balance: making them delightful but not
gimmicky, rich but still accessible. For instance, motion for motion’s sake is avoided – instead, motion is
used to communicate state and provide feedback (a principle long held but now executed with finer detail).
And accessibility settings allow users to reduce motion or increase contrast, so the glitter of these new UIs
doesn’t hinder those with motion sensitivity or low vision. Performance, too, is key – past attempts at flashy
UI (like Windows Vista’s early Aero) were criticized for slowing down devices, something Apple explicitly
addressed by optimizing Liquid Glass for their fast hardware. Thus, the push for physical-feeling UI goes
hand-in-hand with advances in hardware and thoughtful design, ensuring that “alive” interfaces are also
inclusive and efficient.
In Closing: From Command Line to “Living” Interfaces
Looking back, the trajectory of UI history is a fascinating pendulum swing: we started with purely symbolic
interactions (text commands on a screen) and have swung towards interfaces that behave almost like living,
physical entities. Each milestone – the GUI’s desktop metaphor, the adoption of direct manipulation, the
skeuomorphic comfort of early mobile apps, the flat design corrective, and now the resurgence of motion
and depth – was driven by the desire to make the interaction between human and machine more natural.
We’ve learned that humans instinctively understand space, objects, and physics, so the more our digital
interfaces can speak that language, the easier and more delightful they become.
Today’s state-of-the-art UIs, whether it’s a Flutter app following Material 3 Expressive guidelines or an
iPhone running Liquid Glass, are blurring the boundary between the digital and physical. Buttons and
cards on-screen behave with the dynamics and responsiveness we expect of real objects, and in emerging
spatial platforms, they are virtually real objects. As a result, using an app or a website in 2025 can feel less
7
like staring at pixels behind glass and more like interacting with something tangible – something with
weight, texture, and responsiveness.
The history of physical UI is an ongoing story, but its lesson is clear: interfaces aren’t just images or code –
they’re experiences. By grounding those experiences in the familiar rules of our physical world, we make
them more accessible, engaging, and human. And as we move forward – with technologies like Flutter
enabling complex animations and physics on any device, and with AR/VR redefining what a “screen” even is
– designers and developers have more tools than ever to create UIs that truly feel real. The challenge and
excitement now lie in using those tools to craft experiences that not only wow users with delightful details,
but also respect them with thoughtful performance and accessibility. It’s a great time to re-imagine our
apps with a bit of physics and creativity – because the future of UI is not just to look at, but to feel.
1 3
History of the graphical user interface - Wikipedia
https://en.wikipedia.org/wiki/History_of_the_graphical_user_interface
2
Skeuomorph - Wikipedia
https://en.wikipedia.org/wiki/Skeuomorph
4
Physicality: the new age of UI
https://www.lux.camera/physicality-the-new-age-of-ui/
5
Material 3 Expressive deep dive: Features, rollout timeline, supported devices, and more
https://www.androidauthority.com/google-material-3-expressive-features-changes-availability-supported-devices-3556392/
8